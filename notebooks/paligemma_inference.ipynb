{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PaliGemma2 VQA-RAD Inference Notebook\n",
                "\n",
                "This notebook evaluates a fine-tuned PaliGemma2 model on the VQA-RAD test set.\n",
                "Designed to run on Kaggle/Colab with GPU.\n",
                "\n",
                "**Instructions**:\n",
                "1. Upload your `paligemma_best_model` adapter folder\n",
                "2. Upload the VQA-RAD dataset (`VQA_RAD Dataset Public.json` + `VQA_RAD Image Folder`)\n",
                "3. Set the paths in the **Configuration** cell below\n",
                "4. Run all cells"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers accelerate peft bitsandbytes pillow nltk tqdm matplotlib sentence-transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "**Modify these paths according to your environment**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# CONFIGURATION - MODIFY THESE PATHS\n",
                "# =============================================================================\n",
                "\n",
                "# Path to the VQA-RAD annotation JSON file\n",
                "ANNOTATION_PATH = \"/kaggle/input/vqa-rad/VQA_RAD Dataset Public.json\"\n",
                "\n",
                "# Path to the VQA-RAD image folder\n",
                "IMAGE_DIR = \"/kaggle/input/vqa-rad/VQA_RAD Image Folder\"\n",
                "\n",
                "# Path to the fine-tuned adapter checkpoint folder\n",
                "CHECKPOINT_PATH = \"/kaggle/input/paligemma-adapter/paligemma_best_model\"\n",
                "\n",
                "# Base model ID (must match what the adapter was trained on)\n",
                "BASE_MODEL_ID = \"google/paligemma2-3b-pt-224\"\n",
                "\n",
                "# Hugging Face Token (for gated model access)\n",
                "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
                "\n",
                "# Output directory for visualizations\n",
                "OUTPUT_DIR = \"paligemma_qualitative\"\n",
                "\n",
                "# =============================================================================\n",
                "print(f\"Annotation Path: {ANNOTATION_PATH}\")\n",
                "print(f\"Image Directory: {IMAGE_DIR}\")\n",
                "print(f\"Checkpoint Path: {CHECKPOINT_PATH}\")\n",
                "print(f\"Base Model ID: {BASE_MODEL_ID}\")\n",
                "print(f\"Output Directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "from PIL import Image\n",
                "from tqdm.notebook import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "matplotlib.use('Agg')  # For saving figures without display\n",
                "from transformers import (\n",
                "    PaliGemmaForConditionalGeneration,\n",
                "    PaliGemmaProcessor,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import PeftModel\n",
                "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
                "\n",
                "import nltk\n",
                "nltk.download('punkt', quiet=True)\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {DEVICE}\")\n",
                "\n",
                "# Create output directory\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Output directory: {OUTPUT_DIR}/\")\n",
                "\n",
                "# Initialize BioBERT Evaluator for semantic similarity\n",
                "print(\"\\nInitializing BioBERT for semantic evaluation...\")\n",
                "try:\n",
                "    from sentence_transformers import SentenceTransformer, util\n",
                "    biobert_model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO').to(DEVICE)\n",
                "    print(f\" BioBERT loaded on {DEVICE}\")\n",
                "    USE_BIOBERT = True\n",
                "except Exception as e:\n",
                "    print(f\" BioBERT not available: {e}\")\n",
                "    print(\"   Continuing without semantic similarity scores\")\n",
                "    biobert_model = None\n",
                "    util = None\n",
                "    USE_BIOBERT = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_test_data():\n",
                "    \"\"\"Load VQA-RAD test set.\"\"\"\n",
                "    with open(ANNOTATION_PATH) as f:\n",
                "        records = json.load(f)\n",
                "    \n",
                "    # Filter by phrase_type for test set\n",
                "    test_data = [d for d in records if d.get(\"phrase_type\") in [\"test_freeform\", \"test_para\"]]\n",
                "    \n",
                "    closed_test = [r for r in test_data if r.get(\"answer_type\", \"\").lower() == \"closed\"]\n",
                "    open_test = [r for r in test_data if r.get(\"answer_type\", \"\").lower() == \"open\"]\n",
                "    \n",
                "    print(f\"Test set: {len(test_data)} total ({len(closed_test)} closed, {len(open_test)} open)\")\n",
                "    return test_data, closed_test, open_test\n",
                "\n",
                "test_data, closed_test, open_test = load_test_data()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading base model with 4-bit quantization...\")\n",
                "\n",
                "# Quantization config\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16\n",
                ")\n",
                "\n",
                "# Load base model\n",
                "base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
                "    BASE_MODEL_ID,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    token=HF_TOKEN\n",
                ")\n",
                "\n",
                "# Load adapter\n",
                "print(f\"Loading adapter from {CHECKPOINT_PATH}...\")\n",
                "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
                "model.eval()\n",
                "\n",
                "# Load processor\n",
                "processor = PaliGemmaProcessor.from_pretrained(BASE_MODEL_ID, token=HF_TOKEN)\n",
                "\n",
                "print(\" Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_answer(text):\n",
                "    \"\"\"Lowercase, strip, and standardize answer text.\"\"\"\n",
                "    if text is None:\n",
                "        return \"\"\n",
                "    text = str(text).lower().strip()\n",
                "    # Standardize yes/no\n",
                "    if text in [\"yes\", \"y\", \"true\", \"1\"]:\n",
                "        return \"yes\"\n",
                "    if text in [\"no\", \"n\", \"false\", \"0\"]:\n",
                "        return \"no\"\n",
                "    return text\n",
                "\n",
                "\n",
                "def generate_prediction(model, processor, image, question, max_new_tokens=10):\n",
                "    \"\"\"Generate prediction for a single question.\"\"\"\n",
                "    prompt = question  # NO <image> token (matches training format)\n",
                "    \n",
                "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
                "    \n",
                "    decoded = processor.decode(outputs[0], skip_special_tokens=True).lower().strip()\n",
                "    \n",
                "    # Remove echoed question if present\n",
                "    pred_text = decoded.replace(question.lower().strip(), \"\").strip()\n",
                "    \n",
                "    return pred_text\n",
                "\n",
                "\n",
                "def visualize_single_example(image_path, question, prediction, ground_truth, save_path, is_correct):\n",
                "    \"\"\"Create visualization for a single example (EXACT SAN format).\"\"\"\n",
                "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
                "    \n",
                "    # Load and display image\n",
                "    img = Image.open(image_path).convert('RGB')\n",
                "    ax.imshow(img)\n",
                "    ax.axis('off')\n",
                "    \n",
                "    # Add text info at BOTTOM (matching SAN format exactly)\n",
                "    match_color = 'green' if is_correct else 'red'\n",
                "    info_text = f\" Q: {question} \\\\n Prediction: {prediction} | Ground Truth: {ground_truth}\"\n",
                "    fig.text(0.5, 0.02, info_text, ha='center', fontsize=11,\n",
                "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
                "             color=match_color, weight='bold')\n",
                "    \n",
                "    plt.tight_layout(rect=[0, 0.1, 1, 1])  # Leave space for bottom text\n",
                "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "    plt.close()\n",
                "\n",
                "\n",
                "def create_summary_grid(correct, incorrect, save_dir, prefix='summary'):\n",
                "    \"\"\"Create a grid showing 6 correct and 6 incorrect examples (matches SAN format).\"\"\"\n",
                "    fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
                "    \n",
                "    # Top row: Correct predictions\n",
                "    for i in range(6):\n",
                "        if i < len(correct):\n",
                "            ex = correct[i]\n",
                "            img = Image.open(ex['image_path']).convert('RGB')\n",
                "            axes[0, i].imshow(img)\n",
                "            axes[0, i].set_title(f\"✓ Pred: {ex['prediction'][:20]}\", \n",
                "                                fontsize=10, color='green')\n",
                "        axes[0, i].axis('off')\n",
                "    \n",
                "    # Bottom row: Incorrect predictions\n",
                "    for i in range(6):\n",
                "        if i < len(incorrect):\n",
                "            ex = incorrect[i]\n",
                "            img = Image.open(ex['image_path']).convert('RGB')\n",
                "            axes[1, i].imshow(img)\n",
                "            axes[1, i].set_title(f\"✗ Pred: {ex['prediction'][:15]}\\nGT: {ex['ground_truth'][:15]}\", \n",
                "                                fontsize=9, color='red')\n",
                "        axes[1, i].axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    save_path = os.path.join(save_dir, f'{prefix}_grid.png')\n",
                "    plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
                "    plt.close()\n",
                "    print(f\"{prefix.capitalize()} grid saved to {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluate Closed-Ended Questions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_closed(model, processor, test_data):\n",
                "    \"\"\"Evaluate closed-ended (yes/no) questions.\"\"\"\n",
                "    total = 0\n",
                "    correct = 0\n",
                "    \n",
                "    # Store examples for visualization\n",
                "    correct_examples = []\n",
                "    incorrect_examples = []\n",
                "    \n",
                "    for r in tqdm(test_data, desc=\"Closed-Ended\"):\n",
                "        # Load image\n",
                "        img_path = os.path.join(IMAGE_DIR, r[\"image_name\"])\n",
                "        if not os.path.exists(img_path):\n",
                "            continue\n",
                "        image = Image.open(img_path).convert(\"RGB\")\n",
                "        \n",
                "        question = r[\"question\"]\n",
                "        gt_answer = normalize_answer(r[\"answer\"])\n",
                "        \n",
                "        # Generate prediction\n",
                "        pred_text = generate_prediction(model, processor, image, question, max_new_tokens=3)\n",
                "        pred_text = normalize_answer(pred_text.split()[0] if pred_text else \"\")\n",
                "        \n",
                "        total += 1\n",
                "        is_correct = (pred_text == gt_answer)\n",
                "        if is_correct:\n",
                "            correct += 1\n",
                "        \n",
                "        # Store for visualization\n",
                "        example = {\n",
                "            \"image_path\": img_path,\n",
                "            \"question\": question,\n",
                "            \"prediction\": pred_text,\n",
                "            \"ground_truth\": gt_answer,\n",
                "            \"is_correct\": is_correct\n",
                "        }\n",
                "        \n",
                "        if is_correct:\n",
                "            correct_examples.append(example)\n",
                "        else:\n",
                "            incorrect_examples.append(example)\n",
                "    \n",
                "    accuracy = correct / total if total > 0 else 0\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Closed-Ended Results:\")\n",
                "    print(f\"  Total: {total}\")\n",
                "    print(f\"  Correct: {correct}\")\n",
                "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    return {\n",
                "        \"total\": total,\n",
                "        \"correct\": correct,\n",
                "        \"accuracy\": accuracy,\n",
                "        \"correct_examples\": correct_examples,\n",
                "        \"incorrect_examples\": incorrect_examples\n",
                "    }\n",
                "\n",
                "closed_results = evaluate_closed(model, processor, closed_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluate Open-Ended Questions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_open(model, processor, test_data, compute_bleu=True):\n",
                "    \"\"\"Evaluate open-ended questions with BLEU and BioBERT.\"\"\"\n",
                "    total = 0\n",
                "    correct = 0\n",
                "    total_bleu = 0.0\n",
                "    total_biobert = 0.0\n",
                "    biobert_strict = 0  # >0.95\n",
                "    biobert_soft = 0    # >0.85\n",
                "    smooth = SmoothingFunction().method1\n",
                "    \n",
                "    # Store examples for visualization\n",
                "    correct_examples = []\n",
                "    incorrect_examples = []\n",
                "    \n",
                "    for r in tqdm(test_data, desc=\"Open-Ended\"):\n",
                "        # Load image\n",
                "        img_path = os.path.join(IMAGE_DIR, r[\"image_name\"])\n",
                "        if not os.path.exists(img_path):\n",
                "            continue\n",
                "        image = Image.open(img_path).convert(\"RGB\")\n",
                "        \n",
                "        question = r[\"question\"]\n",
                "        gt_answer = normalize_answer(r[\"answer\"])\n",
                "        \n",
                "        # Generate prediction\n",
                "        pred_text = generate_prediction(model, processor, image, question, max_new_tokens=50)\n",
                "        pred_text = normalize_answer(pred_text)\n",
                "        \n",
                "        total += 1\n",
                "        is_correct = (pred_text == gt_answer)\n",
                "        if is_correct:\n",
                "            correct += 1\n",
                "        \n",
                "        # BLEU score\n",
                "        if compute_bleu:\n",
                "            ref_tokens = gt_answer.split()\n",
                "            hyp_tokens = pred_text.split()\n",
                "            bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth)\n",
                "            total_bleu += bleu\n",
                "        \n",
                "        # BioBERT semantic similarity\n",
                "        if USE_BIOBERT and biobert_model is not None:\n",
                "            pred_emb = biobert_model.encode(pred_text, convert_to_tensor=True)\n",
                "            gt_emb = biobert_model.encode(gt_answer, convert_to_tensor=True)\n",
                "            similarity = util.cos_sim(pred_emb, gt_emb).item()\n",
                "            total_biobert += similarity\n",
                "            if similarity > 0.95:\n",
                "                biobert_strict += 1\n",
                "            if similarity > 0.85:\n",
                "                biobert_soft += 1\n",
                "        \n",
                "        # Store for visualization\n",
                "        example = {\n",
                "            \"image_path\": img_path,\n",
                "            \"question\": question,\n",
                "            \"prediction\": pred_text,\n",
                "            \"ground_truth\": gt_answer,\n",
                "            \"is_correct\": is_correct\n",
                "        }\n",
                "        \n",
                "        if is_correct:\n",
                "            correct_examples.append(example)\n",
                "        else:\n",
                "            incorrect_examples.append(example)\n",
                "    \n",
                "    accuracy = correct / total if total > 0 else 0\n",
                "    avg_bleu = total_bleu / total if total > 0 else 0\n",
                "    avg_biobert = total_biobert / total if total > 0 else 0\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Open-Ended Results:\")\n",
                "    print(f\"  Total: {total}\")\n",
                "    print(f\"  Correct (Exact Match): {correct}\")\n",
                "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
                "    if compute_bleu:\n",
                "        print(f\"  Average BLEU: {avg_bleu:.4f}\")\n",
                "    if USE_BIOBERT:\n",
                "        print(f\"  BioBERT Avg Similarity: {avg_biobert:.4f}\")\n",
                "        print(f\"  BioBERT Strict (>0.95): {biobert_strict}\")\n",
                "        print(f\"  BioBERT Soft (>0.85): {biobert_soft}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    return {\n",
                "        \"total\": total,\n",
                "        \"correct\": correct,\n",
                "        \"accuracy\": accuracy,\n",
                "        \"bleu\": avg_bleu,\n",
                "        \"biobert_avg\": avg_biobert if USE_BIOBERT else None,\n",
                "        \"biobert_strict\": biobert_strict if USE_BIOBERT else None,\n",
                "        \"biobert_soft\": biobert_soft if USE_BIOBERT else None,\n",
                "        \"correct_examples\": correct_examples,\n",
                "        \"incorrect_examples\": incorrect_examples\n",
                "    }\n",
                "\n",
                "open_results = evaluate_open(model, processor, open_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Overall Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate overall metrics\n",
                "total_correct = closed_results[\"correct\"] + open_results[\"correct\"]\n",
                "total_questions = closed_results[\"total\"] + open_results[\"total\"]\n",
                "overall_acc = total_correct / total_questions if total_questions > 0 else 0\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"PALIGEMMA EVALUATION - FINAL RESULTS\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Overall Accuracy: {overall_acc:.4f} ({total_correct}/{total_questions})\")\n",
                "print(f\"Closed Accuracy: {closed_results['accuracy']:.4f}\")\n",
                "print(f\"Open Accuracy: {open_results['accuracy']:.4f}\")\n",
                "print(f\"Open BLEU Score: {open_results['bleu']:.4f}\")\n",
                "if USE_BIOBERT and open_results.get('biobert_avg') is not None:\n",
                "    print(f\"BioBERT Avg Similarity: {open_results['biobert_avg']:.4f}\")\n",
                "    print(f\"BioBERT Strict (>0.95): {open_results['biobert_strict']}\")\n",
                "    print(f\"BioBERT Soft (>0.85): {open_results['biobert_soft']}\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "# Save results\n",
                "results = {\n",
                "    \"closed\": {\n",
                "        \"total\": closed_results[\"total\"],\n",
                "        \"correct\": closed_results[\"correct\"],\n",
                "        \"accuracy\": closed_results[\"accuracy\"]\n",
                "    },\n",
                "    \"open\": {\n",
                "        \"total\": open_results[\"total\"],\n",
                "        \"correct\": open_results[\"correct\"],\n",
                "        \"accuracy\": open_results[\"accuracy\"],\n",
                "        \"bleu\": open_results[\"bleu\"],\n",
                "        \"biobert_avg\": open_results.get(\"biobert_avg\"),\n",
                "        \"biobert_strict\": open_results.get(\"biobert_strict\"),\n",
                "        \"biobert_soft\": open_results.get(\"biobert_soft\")\n",
                "    },\n",
                "    \"overall\": {\n",
                "        \"total\": total_questions,\n",
                "        \"correct\": total_correct,\n",
                "        \"accuracy\": overall_acc\n",
                "    }\n",
                "}\n",
                "\n",
                "results_path = os.path.join(OUTPUT_DIR, \"evaluation_results.json\")\n",
                "with open(results_path, \"w\") as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "    \n",
                "print(f\"\\n Results saved to {results_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Generate Qualitative Visualizations (SAN Format)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"GENERATING QUALITATIVE VISUALIZATIONS (SAN Format)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "num_examples = 10  # Save top 10 of each category\n",
                "\n",
                "# ===== CLOSED-ENDED VISUALIZATIONS =====\n",
                "print(\"\\nGenerating visualizations for CLOSED-ENDED questions...\")\n",
                "\n",
                "print(\"  Saving correct predictions...\")\n",
                "for i, ex in enumerate(closed_results['correct_examples'][:num_examples]):\n",
                "    save_path = os.path.join(OUTPUT_DIR, f'closed_correct_{i+1}.png')\n",
                "    visualize_single_example(\n",
                "        ex['image_path'], ex['question'], ex['prediction'], \n",
                "        ex['ground_truth'], save_path, is_correct=True\n",
                "    )\n",
                "print(f\"    ✓ Saved {min(num_examples, len(closed_results['correct_examples']))} correct examples\")\n",
                "\n",
                "print(\"  Saving incorrect predictions...\")\n",
                "for i, ex in enumerate(closed_results['incorrect_examples'][:num_examples]):\n",
                "    save_path = os.path.join(OUTPUT_DIR, f'closed_incorrect_{i+1}.png')\n",
                "    visualize_single_example(\n",
                "        ex['image_path'], ex['question'], ex['prediction'], \n",
                "        ex['ground_truth'], save_path, is_correct=False\n",
                "    )\n",
                "print(f\"    ✓ Saved {min(num_examples, len(closed_results['incorrect_examples']))} incorrect examples\")\n",
                "\n",
                "# ===== OPEN-ENDED VISUALIZATIONS =====\n",
                "print(\"\\nGenerating visualizations for OPEN-ENDED questions...\")\n",
                "\n",
                "print(\"  Saving correct predictions...\")\n",
                "for i, ex in enumerate(open_results['correct_examples'][:num_examples]):\n",
                "    save_path = os.path.join(OUTPUT_DIR, f'open_correct_{i+1}.png')\n",
                "    visualize_single_example(\n",
                "        ex['image_path'], ex['question'], ex['prediction'], \n",
                "        ex['ground_truth'], save_path, is_correct=True\n",
                "    )\n",
                "print(f\"    ✓ Saved {min(num_examples, len(open_results['correct_examples']))} correct examples\")\n",
                "\n",
                "print(\"  Saving incorrect predictions...\")\n",
                "for i, ex in enumerate(open_results['incorrect_examples'][:num_examples]):\n",
                "    save_path = os.path.join(OUTPUT_DIR, f'open_incorrect_{i+1}.png')\n",
                "    visualize_single_example(\n",
                "        ex['image_path'], ex['question'], ex['prediction'], \n",
                "        ex['ground_truth'], save_path, is_correct=False\n",
                "    )\n",
                "print(f\"    ✓ Saved {min(num_examples, len(open_results['incorrect_examples']))} incorrect examples\")\n",
                "\n",
                "# ===== SUMMARY GRIDS =====\n",
                "print(\"\\nCreating summary grids...\")\n",
                "create_summary_grid(\n",
                "    closed_results['correct_examples'][:6], \n",
                "    closed_results['incorrect_examples'][:6], \n",
                "    OUTPUT_DIR, 'closed'\n",
                ")\n",
                "create_summary_grid(\n",
                "    open_results['correct_examples'][:6], \n",
                "    open_results['incorrect_examples'][:6], \n",
                "    OUTPUT_DIR, 'open'\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(f\"Qualitative analysis complete! All results saved to {OUTPUT_DIR}/\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
