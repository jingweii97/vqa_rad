{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"medgemma_inference.ipynb\n",
    "\n",
    "MedGemma Zero-Shot VQA-RAD Inference Notebook\n",
    "\n",
    "This notebook evaluates MedGemma-4B on the VQA-RAD test set using zero-shot prompting.\n",
    "Designed to run on Kaggle/Colab with GPU.\n",
    "\n",
    "**Instructions**:\n",
    "1. Upload the VQA-RAD dataset (`VQA_RAD Dataset Public.json` + `VQA_RAD Image Folder`)\n",
    "2. Set the paths in the **Configuration** cell below\n",
    "3. Run all cells\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4137955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## 1. Install Dependencies\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes pillow nltk tqdm matplotlib sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e71ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## 2. Configuration\n",
    "**Modify these paths according to your environment**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a16247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc0e61",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "CONFIGURATION - MODIFY THESE PATHS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the VQA-RAD annotation JSON file\n",
    "INPUT_DIR = \"/content/drive/MyDrive/vqa-rad/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04524fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_PATH = os.path.join(INPUT_DIR, \"input/VQA_RAD Dataset Public.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the VQA-RAD image folder\n",
    "IMAGE_DIR = os.path.join(INPUT_DIR, \"input/VQA_RAD Image Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID (MedGemma 4B Instruct)\n",
    "MODEL_ID = \"google/medgemma-4b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join(INPUT_DIR, \"medgemma_qualitative/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe403b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aff89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Token (for gated model access)\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Loaded HF_TOKEN from Colab secrets\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load HF_TOKEN from Colab secrets: {e}\")\n",
    "    print(\"üëâ Tip: Add 'HF_TOKEN' to the Secrets tab (key icon) on the left of Colab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8cafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "print(f\"Annotation Path: {ANNOTATION_PATH}\")\n",
    "print(f\"Image Directory: {IMAGE_DIR}\")\n",
    "print(f\"Model ID: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaba96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## 3. Imports\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e830328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # For saving figures without display\n",
    "from transformers import (\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f16df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BioBERT Evaluator for semantic similarity\n",
    "print(\"\\nInitializing BioBERT for semantic evaluation...\")\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    biobert_model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO').to(DEVICE)\n",
    "    print(f\"‚úÖ BioBERT loaded on {DEVICE}\")\n",
    "    USE_BIOBERT = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  BioBERT not available: {e}\")\n",
    "    print(\"   Continuing without semantic similarity scores\")\n",
    "    biobert_model = None\n",
    "    util = None\n",
    "    USE_BIOBERT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16411d5b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## 4. Load Dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743764c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    \"\"\"Load VQA-RAD test set.\"\"\"\n",
    "    with open(ANNOTATION_PATH) as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    # Filter by phrase_type for test set\n",
    "    test_data = [d for d in records if d.get(\"phrase_type\") in [\"test_freeform\", \"test_para\"]]\n",
    "\n",
    "    closed_test = [r for r in test_data if r.get(\"answer_type\", \"\").lower() == \"closed\"]\n",
    "    open_test = [r for r in test_data if r.get(\"answer_type\", \"\").lower() == \"open\"]\n",
    "\n",
    "    print(f\"Test set: {len(test_data)} total ({len(closed_test)} closed, {len(open_test)} open)\")\n",
    "    return test_data, closed_test, open_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, closed_test, open_test = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## 5. Load Model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c48643",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MedGemma with 4-bit quantization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07dc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b936049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## 6. Helper Functions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot template for open-ended questions (from MedGemma notebook)\n",
    "FEW_SHOT_TEMPLATE = \"\"\"Provide a short, concise answer to this radiology question.\n",
    "Only answer in short words or phrases (e.g., 'pancreas', 'free air', 'diffuse', 'posterior to the appendix', 'left hepatic lobe')\n",
    "Do not write a sentence. Follow these examples:\n",
    "\n",
    "Q: What modality is shown?\n",
    "A: x-ray\n",
    "\n",
    "Q: Where is the opacity located?\n",
    "A: right upper lobe\n",
    "\n",
    "Q: How would you describe the spleen abnormality?\n",
    "A: hypodense lesion\n",
    "\n",
    "Q: What is the abnormality seen?\n",
    "A: pleural effusion\n",
    "\n",
    "Q: What is the plane of this image?\n",
    "A: axial\n",
    "\n",
    "Q: How big is the liver lesion?\n",
    "A: 5 cm\n",
    "\n",
    "Now, answer this question for the image provided:\n",
    "Q: {question}\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b39402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(text):\n",
    "    \"\"\"Lowercase, strip, and standardize answer text.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text).lower().strip()\n",
    "    # Standardize yes/no\n",
    "    if text in [\"yes\", \"y\", \"true\", \"1\"]:\n",
    "        return \"yes\"\n",
    "    if text in [\"no\", \"n\", \"false\", \"0\"]:\n",
    "        return \"no\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_closed_prediction(model, processor, image, question):\n",
    "    \"\"\"Generate prediction for a closed-ended question.\"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": f\"Answer the following question with exactly one word (e.g., 'yes', 'no', 'left', 'right', 'ct', 'mri'). Do not write a sentence. Question: {question}\"}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text_prompt], images=[image], return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=5,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Decode and clean\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    pred_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n",
    "    pred_text = \"\".join([c for c in pred_text if c.isalnum() or c.isspace()])\n",
    "\n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_open_prediction(model, processor, image, question):\n",
    "    \"\"\"Generate prediction for an open-ended question.\"\"\"\n",
    "    prompt_text = FEW_SHOT_TEMPLATE.format(question=question)\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": prompt_text}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text_prompt], images=[image], return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and clean\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    pred_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n",
    "    pred_text = pred_text.replace(\"a:\", \"\").strip()\n",
    "\n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9160c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_example(image_path, question, prediction, ground_truth, save_path, is_correct):\n",
    "    \"\"\"Create visualization for a single example (EXACT SAN format).\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    # Load and display image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Add text info at BOTTOM (matching SAN format exactly)\n",
    "    match_color = 'green' if is_correct else 'red'\n",
    "    info_text = f\" Q: {question} \\n Prediction: {prediction} | Ground Truth: {ground_truth}\"\n",
    "    fig.text(0.5, 0.02, info_text, ha='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "             color=match_color, weight='bold')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 1])  # Leave space for bottom text\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d998b33",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_summary_grid(correct, incorrect, save_dir, prefix='summary'):\n",
    "    \"\"\"Create a grid showing 6 correct and 6 incorrect examples (matches SAN format).\"\"\"\n",
    "    fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "\n",
    "    # Top row: Correct predictions\n",
    "    for i in range(6):\n",
    "        if i < len(correct):\n",
    "            ex = correct[i]\n",
    "            img = Image.open(ex['image_path']).convert('RGB')\n",
    "            axes[0, i].imshow(img)\n",
    "            axes[0, i].set_title(f\"‚úì Pred: {ex['prediction'][:20]}\",\n",
    "                                fontsize=10, color='green')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "    # Bottom row: Incorrect predictions\n",
    "    for i in range(6):\n",
    "        if i < len(incorrect):\n",
    "            ex = incorrect[i]\n",
    "            img = Image.open(ex['image_path']).convert('RGB')\n",
    "            axes[1, i].imshow(img)\n",
    "            axes[1, i].set_title(f\"‚úó Pred: {ex['prediction'][:15]}\\nGT: {ex['ground_truth'][:15]}\",\n",
    "                                fontsize=9, color='red')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'{prefix}_grid.png')\n",
    "    plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"{prefix.capitalize()} grid saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d586f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## 7. Evaluate Closed-Ended Questions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2674db6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_closed(model, processor, test_data):\n",
    "    \"\"\"Evaluate closed-ended (yes/no) questions.\"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    correct_examples = []\n",
    "    incorrect_examples = []\n",
    "\n",
    "    for r in tqdm(test_data, desc=\"Closed-Ended\"):\n",
    "        img_path = os.path.join(IMAGE_DIR, r[\"image_name\"])\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        question = r[\"question\"]\n",
    "        gt_answer = normalize_answer(r[\"answer\"])\n",
    "\n",
    "        pred_text = generate_closed_prediction(model, processor, image, question)\n",
    "        pred_text = normalize_answer(pred_text.split()[0] if pred_text else \"\")\n",
    "\n",
    "        total += 1\n",
    "        is_correct = (pred_text == gt_answer)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        example = {\n",
    "            \"image_path\": img_path,\n",
    "            \"question\": question,\n",
    "            \"prediction\": pred_text,\n",
    "            \"ground_truth\": gt_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "\n",
    "        if is_correct:\n",
    "            correct_examples.append(example)\n",
    "        else:\n",
    "            incorrect_examples.append(example)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Closed-Ended Results:\")\n",
    "    print(f\"  Total: {total}\")\n",
    "    print(f\"  Correct: {correct}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"correct\": correct,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct_examples\": correct_examples,\n",
    "        \"incorrect_examples\": incorrect_examples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f558b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_results = evaluate_closed(model, processor, closed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838fef8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## 8. Evaluate Open-Ended Questions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8efbaa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_open(model, processor, test_data, compute_bleu=True):\n",
    "    \"\"\"Evaluate open-ended questions with BLEU and BioBERT.\"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_bleu = 0.0\n",
    "    total_biobert = 0.0\n",
    "    biobert_strict = 0\n",
    "    biobert_soft = 0\n",
    "    smooth = SmoothingFunction().method1\n",
    "\n",
    "    correct_examples = []\n",
    "    incorrect_examples = []\n",
    "\n",
    "    for r in tqdm(test_data, desc=\"Open-Ended\"):\n",
    "        img_path = os.path.join(IMAGE_DIR, r[\"image_name\"])\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        question = r[\"question\"]\n",
    "        gt_answer = normalize_answer(r[\"answer\"])\n",
    "\n",
    "        pred_text = generate_open_prediction(model, processor, image, question)\n",
    "        pred_text = normalize_answer(pred_text)\n",
    "\n",
    "        total += 1\n",
    "        is_correct = (pred_text == gt_answer)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        if compute_bleu:\n",
    "            ref_tokens = gt_answer.split()\n",
    "            hyp_tokens = pred_text.split()\n",
    "            bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth)\n",
    "            total_bleu += bleu\n",
    "\n",
    "        if USE_BIOBERT and biobert_model is not None:\n",
    "            pred_emb = biobert_model.encode(pred_text, convert_to_tensor=True)\n",
    "            gt_emb = biobert_model.encode(gt_answer, convert_to_tensor=True)\n",
    "            similarity = util.cos_sim(pred_emb, gt_emb).item()\n",
    "            total_biobert += similarity\n",
    "            if similarity > 0.95:\n",
    "                biobert_strict += 1\n",
    "            if similarity > 0.85:\n",
    "                biobert_soft += 1\n",
    "\n",
    "        example = {\n",
    "            \"image_path\": img_path,\n",
    "            \"question\": question,\n",
    "            \"prediction\": pred_text,\n",
    "            \"ground_truth\": gt_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "\n",
    "        if is_correct:\n",
    "            correct_examples.append(example)\n",
    "        else:\n",
    "            incorrect_examples.append(example)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_bleu = total_bleu / total if total > 0 else 0\n",
    "    avg_biobert = total_biobert / total if total > 0 else 0\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Open-Ended Results:\")\n",
    "    print(f\"  Total: {total}\")\n",
    "    print(f\"  Correct (Exact Match): {correct}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    if compute_bleu:\n",
    "        print(f\"  Average BLEU: {avg_bleu:.4f}\")\n",
    "    if USE_BIOBERT:\n",
    "        print(f\"  BioBERT Avg Similarity: {avg_biobert:.4f}\")\n",
    "        print(f\"  BioBERT Strict (>0.95): {biobert_strict}\")\n",
    "        print(f\"  BioBERT Soft (>0.85): {biobert_soft}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"correct\": correct,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"biobert_avg\": avg_biobert if USE_BIOBERT else None,\n",
    "        \"biobert_strict\": biobert_strict if USE_BIOBERT else None,\n",
    "        \"biobert_soft\": biobert_soft if USE_BIOBERT else None,\n",
    "        \"correct_examples\": correct_examples,\n",
    "        \"incorrect_examples\": incorrect_examples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_results = evaluate_open(model, processor, open_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## 9. Overall Results\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = closed_results[\"correct\"] + open_results[\"correct\"]\n",
    "total_questions = closed_results[\"total\"] + open_results[\"total\"]\n",
    "overall_acc = total_correct / total_questions if total_questions > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MEDGEMMA EVALUATION - FINAL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Overall Accuracy: {overall_acc:.4f} ({total_correct}/{total_questions})\")\n",
    "print(f\"Closed Accuracy: {closed_results['accuracy']:.4f}\")\n",
    "print(f\"Open Accuracy: {open_results['accuracy']:.4f}\")\n",
    "print(f\"Open BLEU Score: {open_results['bleu']:.4f}\")\n",
    "if USE_BIOBERT and open_results.get('biobert_avg') is not None:\n",
    "    print(f\"BioBERT Avg Similarity: {open_results['biobert_avg']:.4f}\")\n",
    "    print(f\"BioBERT Strict (>0.95): {open_results['biobert_strict']}\")\n",
    "    print(f\"BioBERT Soft (>0.85): {open_results['biobert_soft']}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5daefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"closed\": {\n",
    "        \"total\": closed_results[\"total\"],\n",
    "        \"correct\": closed_results[\"correct\"],\n",
    "        \"accuracy\": closed_results[\"accuracy\"]\n",
    "    },\n",
    "    \"open\": {\n",
    "        \"total\": open_results[\"total\"],\n",
    "        \"correct\": open_results[\"correct\"],\n",
    "        \"accuracy\": open_results[\"accuracy\"],\n",
    "        \"bleu\": open_results[\"bleu\"],\n",
    "        \"biobert_avg\": open_results.get(\"biobert_avg\"),\n",
    "        \"biobert_strict\": open_results.get(\"biobert_strict\"),\n",
    "        \"biobert_soft\": open_results.get(\"biobert_soft\")\n",
    "    },\n",
    "    \"overall\": {\n",
    "        \"total\": total_questions,\n",
    "        \"correct\": total_correct,\n",
    "        \"accuracy\": overall_acc\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a957c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(OUTPUT_DIR, \"evaluation_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657061f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n‚úÖ Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f33556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## 10. Qualitative Visualizations\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900999b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING QUALITATIVE VISUALIZATIONS (SAN Format)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae73294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating visualizations for CLOSED-ENDED questions...\")\n",
    "print(\"  Saving correct predictions...\")\n",
    "for i, ex in enumerate(closed_results['correct_examples'][:num_examples]):\n",
    "    save_path = os.path.join(OUTPUT_DIR, f'closed_correct_{i+1}.png')\n",
    "    visualize_single_example(\n",
    "        ex['image_path'], ex['question'], ex['prediction'],\n",
    "        ex['ground_truth'], save_path, is_correct=True\n",
    "    )\n",
    "print(f\"    ‚úì Saved {min(num_examples, len(closed_results['correct_examples']))} correct examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df578009",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Saving incorrect predictions...\")\n",
    "for i, ex in enumerate(closed_results['incorrect_examples'][:num_examples]):\n",
    "    save_path = os.path.join(OUTPUT_DIR, f'closed_incorrect_{i+1}.png')\n",
    "    visualize_single_example(\n",
    "        ex['image_path'], ex['question'], ex['prediction'],\n",
    "        ex['ground_truth'], save_path, is_correct=False\n",
    "    )\n",
    "print(f\"    ‚úì Saved {min(num_examples, len(closed_results['incorrect_examples']))} incorrect examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56824ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating visualizations for OPEN-ENDED questions...\")\n",
    "print(\"  Saving correct predictions...\")\n",
    "for i, ex in enumerate(open_results['correct_examples'][:num_examples]):\n",
    "    save_path = os.path.join(OUTPUT_DIR, f'open_correct_{i+1}.png')\n",
    "    visualize_single_example(\n",
    "        ex['image_path'], ex['question'], ex['prediction'],\n",
    "        ex['ground_truth'], save_path, is_correct=True\n",
    "    )\n",
    "print(f\"    ‚úì Saved {min(num_examples, len(open_results['correct_examples']))} correct examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Saving incorrect predictions...\")\n",
    "for i, ex in enumerate(open_results['incorrect_examples'][:num_examples]):\n",
    "    save_path = os.path.join(OUTPUT_DIR, f'open_incorrect_{i+1}.png')\n",
    "    visualize_single_example(\n",
    "        ex['image_path'], ex['question'], ex['prediction'],\n",
    "        ex['ground_truth'], save_path, is_correct=False\n",
    "    )\n",
    "print(f\"    ‚úì Saved {min(num_examples, len(open_results['incorrect_examples']))} incorrect examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating summary grids...\")\n",
    "create_summary_grid(\n",
    "    closed_results['correct_examples'][:6],\n",
    "    closed_results['incorrect_examples'][:6],\n",
    "    OUTPUT_DIR, 'closed'\n",
    ")\n",
    "create_summary_grid(\n",
    "    open_results['correct_examples'][:6],\n",
    "    open_results['incorrect_examples'][:6],\n",
    "    OUTPUT_DIR, 'open'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425df9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ Qualitative analysis complete! All results saved to {OUTPUT_DIR}/\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
